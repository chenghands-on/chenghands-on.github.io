@article{Xie_Li_Zhang_Deng_Ge_Ye_2024, title={Trust Region Methods for Nonconvex Stochastic Optimization beyond Lipschitz Smoothness}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29537}, DOI={10.1609/aaai.v38i14.29537}, abstractNote={In many important machine learning applications, the standard assumption of having a globally Lipschitz continuous gradient may fail to hold. This paper delves into a more general (L0, L1)-smoothness setting, which gains particular significance within the realms of deep neural networks and distributionally robust optimization (DRO). We demonstrate the significant advantage of trust region methods for stochastic nonconvex optimization under such generalized smoothness assumption. We show that first-order trust region methods can recover the normalized and clipped stochastic gradient as special cases and then provide a unified analysis to show their convergence to first-order stationary conditions. Motivated by the important application of DRO, we propose a generalized high-order smoothness condition, under which second-order trust region methods can achieve a complexity of O(epsilon(-3.5)) for convergence to second-order stationary points. By incorporating variance reduction, the second-order trust region method obtains an even better complexity of O(epsilon(-3)), matching the optimal bound for standard smooth optimization. To our best knowledge, this is the first work to show convergence beyond the first-order stationary condition for generalized smooth optimization. Preliminary experiments show that our proposed algorithms perform favorably compared with existing methods.}, number={14}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xie, Chenghan and Li, Chenxi and Zhang, Chuwen and Deng, Qi and Ge, Dongdong and Ye, Yinyu}, year={2024}, month={Mar.}, pages={16049-16057} }
@article{Liu_Xie_Deng_Ge_Ye_2024, title={Sketched Newton Value Iteration for Large-Scale Markov Decision Processes}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29301}, DOI={10.1609/aaai.v38i12.29301}, abstractNote={Value Iteration (VI) is one of the most classic algorithms for solving Markov Decision Processes (MDPs), which lays the foundations for various more advanced reinforcement learning algorithms, such as Q-learning. VI may take a large number of iterations to converge as it is a first-order method. In this paper, we introduce the Newton Value Iteration (NVI) algorithm, which eliminates the impact of action space dimension compared to some previous second-order methods. Consequently, NVI can efficiently handle MDPs with large action spaces. Building upon NVI, we propose a novel approach called Sketched Newton Value Iteration (SNVI) to tackle MDPs with both large state and action spaces. SNVI not only inherits the stability and fast convergence advantages of second-order algorithms, but also significantly reduces computational complexity, making it highly scalable. Extensive experiments demonstrate the superiority of our algorithms over traditional VI and previously proposed second-order VI algorithms.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Jinsong and Xie, Chenghan and Deng, Qi and Ge, Dongdong and Ye, Yinyu}, year={2024}, month={Mar.}, pages={13936-13944} }